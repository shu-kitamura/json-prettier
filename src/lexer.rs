use std::{
    iter::Peekable,
    str::Chars
};
use crate::error::{JsonPretError, LexerError};

#[derive(Debug, PartialEq, Clone)]
enum Token {
    String(String), // æ–‡å­—åˆ—
    Number(f64),    // æ•°å€¤
    Bool(bool),     // çœŸå½å€¤
    Null,           // Null
    WhiteSpace,     // ç©ºç™½
    LeftBrace,      // {ã€€JSON object é–‹å§‹æ–‡å­—
    RightBrace,     // }ã€€JSON object çµ‚äº†æ–‡å­—
    LeftBracket,    // [ã€€JSON array  é–‹å§‹æ–‡å­—
    RightBracket,   // ]ã€€JSON array  çµ‚äº†æ–‡å­—
    Comma,          // ,ã€€JSON value  åŒºåˆ‡ã‚Šæ–‡å­—
    Colon,          // :ã€€"key":value åŒºåˆ‡ã‚Šæ–‡å­—
}

#[derive(Debug)]
struct Lexer<'a> {
    chars: Peekable<Chars<'a>>
}

impl<'a> Lexer<'a> {
    fn new(raw_str: &str) -> Lexer {
        Lexer {
            chars: raw_str.chars().peekable()
        }
    }

    fn lexical_analyze(&mut self) -> Result<Vec<Token>, JsonPretError> {
        let mut tokens: Vec<Token> = vec![];
        while let Some(token) = self.next_token().unwrap() {
            match token {
                Token::WhiteSpace => {}
                _ => tokens.push(token),
            }
        }
        Ok(tokens)
    }

    /// æ–‡å­—åˆ—ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒƒãƒã—ãŸTokenã‚’è¿”ã™
    fn next_token(&mut self) -> Result<Option<Token>, JsonPretError> {
        match self.chars.peek() {
            Some(c) => match c {
                c if c.is_whitespace() || *c == '\n' => Ok(Some(self.get_token(Token::WhiteSpace))),
                c if is_number(*c, true) => Ok(Some(self.parse_number().unwrap())),
                '{' => Ok(Some(self.get_token(Token::LeftBrace))),
                '}' => Ok(Some(self.get_token(Token::RightBrace))),
                '[' => Ok(Some(self.get_token(Token::LeftBracket))),
                ']' => Ok(Some(self.get_token(Token::RightBracket))),
                ',' => Ok(Some(self.get_token(Token::Comma))),
                ':' => Ok(Some(self.get_token(Token::Colon))),
                '"' => Ok(Some(self.parse_string().unwrap())),
                't' => Ok(Some(self.parse_boolean(true).unwrap())),
                'f' => Ok(Some(self.parse_boolean(false).unwrap())),
                'n' => Ok(Some(self.parse_null().unwrap())),
                _ => Err(JsonPretError::LexerError(
                    LexerError::new(&format!("an unexpected char {}", c))
                )),
            }, 
            None => Ok(None)
        }
    }

    fn get_token(&mut self, token: Token) -> Token {
        self.chars.next();
        token
    } 

    fn parse_number(&mut self) -> Result<Token, JsonPretError>{
        let mut number_str: String = String::new();
        while let Some(&c) = self.chars.peek() {
            if is_number(c, false) {
                self.chars.next();
                number_str.push(c);
            } else {
                break;
            }
        }

        match number_str.parse::<f64>() {
            Ok(number) => Ok(Token::Number(number)),
            Err(e) => Err(JsonPretError::LexerError(
                LexerError::new(&e.to_string()),
            ))
        }
    }

    fn parse_boolean(&mut self, b: bool) -> Result<Token, JsonPretError> {
        // true ã®å ´åˆã¯4æ–‡å­—ã€falseã®å ´åˆã¯5æ–‡å­—å–å¾—
        let string: String =  match b {
            true => self.get_string(4),
            false => self.get_string(5),
        };

        if &string == "true" || &string == "false" {
            Ok(Token::Bool(b))
        } else {
            Err(JsonPretError::LexerError(
                LexerError::new(&format!("'{string}' is syntactically incorrect."))
            ))
        }
    }

    fn parse_null(&mut self) -> Result<Token, JsonPretError> {
        // 4æ–‡å­—å–å¾—
        let string: String = self.get_string(4);
        
        // èª­ã¿è¾¼ã‚“ã æ–‡å­—ãŒ "null" ã®å ´åˆã€Token ã‚’è¿”ã™ã€‚
        if &string == "null" {
            Ok(Token::Null)
        } else {
            Err(JsonPretError::LexerError(
                LexerError::new(&format!("'{string}' is syntactically incorrect."))
            ))
        }
    }

    fn parse_string(&mut self) -> Result<Token, JsonPretError>{
        self.chars.next(); // æœ€åˆã® " ã®åˆ†ã‚’é€²ã‚ã‚‹ã€‚

        let mut utf16: Vec<u16> = vec![];
        let mut string: String = String::new();

        while let Some(c) = self.chars.next() {
            match c {
                '\\' => {
                    let escaped_c = self.chars.next().unwrap();
                    println!("{escaped_c}");
                    match escaped_c {
                        '"' | '\\' | '/' | 'b' | 'f' | 'n' | 'r' | 't' => {
                            // ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—æ–‡å­—ã®æ™‚ã®å‡¦ç†
                            match self.push_utf16(&mut string, &mut utf16) {
                                Ok(()) => string.push_str(&format!("\\{escaped_c}")),
                                Err(e) => return Err(e)
                            }
                        }
                        'u' => {
                            // utf16ã®æ™‚ã®å‡¦ç†
                            let code_point = match self.get_code_point() {
                                Ok(point) => point,
                                Err(e) => return Err(e),
                            };
                            utf16.push(code_point);
                        }
                        _ => return Err(JsonPretError::LexerError(
                            LexerError::new(&format!("an unexpected escaped char {escaped_c}"))
                        ))
                    }
                }
                '\"' => {
                    // æ–‡å­—åˆ—ãƒ‘ãƒ¼ã‚¹ã®çµ‚äº†æ™‚ã®å‡¦ç†
                    match self.push_utf16(&mut string, &mut utf16) {
                        Ok(_) => break,
                        Err(e) => return Err(e)
                    }
                },
                _ => {
                    // æ™®é€šã®æ–‡å­—ã®æ™‚ã®å‡¦ç†
                    match self.push_utf16(&mut string, &mut utf16) {
                        Ok(_) => string.push(c),
                        Err(e) => return Err(e)
                    }

                }
            }
        }
        Ok(Token::String(string))
    }

    /// æŒ‡å®šã—ãŸæ–‡å­—æ•°ã‚’å–å¾—ã™ã‚‹
    fn get_string(&mut self, length: usize) -> String {
        let mut string: String = String::new();
        for _ in 0..length {
            match self.chars.next() {
                Some(c) => string.push(c),
                None => {}
            }
        }
        string
    }

    /// utf16ã®ã‚³ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹
    fn get_code_point(&mut self) -> Result<u16, JsonPretError> {
        let hexs = (0..4).filter_map(|_| {
            let c: char = self.chars.next().unwrap();
            if c.is_ascii_hexdigit() {
                Some(c)
            } else {
                None
            }
        });

        // èª­ã¿è¾¼ã‚“ã æ–‡å­—åˆ—ã‚’16æ–°æ•°ã«å¤‰æ›ã—ã¦ã€utf16ã®ãƒãƒƒãƒ•ã‚¡ã«pushã™ã‚‹
        match u16::from_str_radix(&hexs.collect::<String>(), 16) {
            Ok(code_point) => Ok(code_point),
            Err(e) => Err(JsonPretError::LexerError(
                LexerError::new(&e.to_string())
            ))
        }
    }
    /// utf16ã®ãƒãƒƒãƒ•ã‚¡ã‚’æ–‡å­—åˆ—ã«çµåˆã™ã‚‹
    fn push_utf16(&mut self, string: &mut String, utf16: &mut Vec<u16>) -> Result<(), JsonPretError>{
        if utf16.is_empty() {
            return Ok(());
        }

        match String::from_utf16(utf16) {
            Ok(utf16_str) => {
                string.push_str(&utf16_str);
                utf16.clear();
                Ok(())
            }
            Err(e) => return Err(JsonPretError::LexerError(
                LexerError::new(&e.to_string())
            ))
        }
    }
}

/// Numberã§ä½¿ç”¨ã•ã‚Œã‚‹æ–‡å­—([0-9], +, -, .)ã‹ã©ã†ã‹ã‚’è¿”ã™ã€‚  
fn is_number(c: char, is_prefix: bool) -> bool {
    if is_prefix {
        c.is_numeric() || matches!(c, '+' | '-' | '.')
    } else {
        c.is_numeric() || matches!(c, '+' | '-' | 'e' | 'E' | '.')

    }
}

// --- ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ ---

#[cfg(test)]
mod tests {
    use crate::{error::{JsonPretError, LexerError}, lexer::{Lexer, Token, is_number}};

    #[test]
    fn test_lexer_new() {
        let expect = Lexer {
            chars: r##"{"key" : "value}"##.chars().peekable()
        };

        let actual = Lexer::new(r##"{"key" : "value}"##);
        for (ac, ec) in actual.chars.zip(expect.chars) {
            assert_eq!(ac, ec);
        }
    }

    // #[test]
    // fn test_next_token() {
    //     let expect = Token::LeftBrace;
    // }

    #[test]
    fn test_parse_number() {
        let expect = Token::Number(1.0);
        let mut lexer = Lexer::new("1.0");
        let actual = lexer.parse_number().unwrap();

        assert_eq!(actual, expect)
    }
 
    #[test]
    fn test_parse_boolean() {
        // true ã®ã‚±ãƒ¼ã‚¹
        let expect_true = Token::Bool(true);
        let mut lexer_true = Lexer::new("true");
        let actual_true = lexer_true.parse_boolean(true).unwrap();
        assert_eq!(actual_true, expect_true);

        // false ã®ã‚±ãƒ¼ã‚¹
        let expect_false = Token::Bool(false);
        let mut lexer_false = Lexer::new("false");
        let actual_false = lexer_false.parse_boolean(false).unwrap();
        assert_eq!(actual_false, expect_false);

        // t ã§ true ä»¥å¤–ã®æ–‡å­—ã®ã‚±ãƒ¼ã‚¹(ã‚¨ãƒ©ãƒ¼)
        let err_str_t = "test";
        let expect_err_t = JsonPretError::LexerError(
            LexerError::new(&format!("'{err_str_t}' is syntactically incorrect."))
        );
        let mut lexer_err_t = Lexer::new(&err_str_t);
        let actual_err_t = lexer_err_t.parse_boolean(true).unwrap_err();
        assert_eq!(actual_err_t, expect_err_t);

        // f ã§ false ä»¥å¤–ã®æ–‡å­—ã®ã‚±ãƒ¼ã‚¹(ã‚¨ãƒ©ãƒ¼)
        let err_str_f = "fight";
        let expect_err_f = JsonPretError::LexerError(
            LexerError::new(&format!("'{err_str_f}' is syntactically incorrect."))
        );
        let mut lexer_err_f = Lexer::new(&err_str_f);
        let actual_err_f = lexer_err_f.parse_boolean(false).unwrap_err();
        assert_eq!(actual_err_f, expect_err_f);
    }

    #[test]
    fn test_parse_null() {
        let expect = Token::Null;
        let mut lexer = Lexer::new("null");        
        let actual = lexer.parse_null().unwrap();

        assert_eq!(actual, expect);
    }

    #[test]
    fn test_parse_string() {
        let s = "\"hogehoge12345\"";
        let token = Lexer::new(s).parse_string().unwrap();
        assert_eq!(token, Token::String("hogehoge12345".to_string()));

        let s = "\"ã‚ã„ã†ãˆãŠ\"";
        let token = Lexer::new(s).parse_string().unwrap();
        assert_eq!(token, Token::String("ã‚ã„ã†ãˆãŠ".to_string()));

        let s = r#""\u3042\u3044\u3046abc""#; //ã‚ã„ã†abc
        let token = Lexer::new(s).parse_string().unwrap();
        assert_eq!(token, Token::String("ã‚ã„ã†abc".to_string()));

        let s = format!(r#"\\b\f\n\r\t\/\""#);
        let token = Lexer::new(&s).parse_string().unwrap();
        assert_eq!(
            token,
            Token::String(r#"\b\f\n\r\t\/\""#.to_string())
        );

        let s = r#""\uD83D\uDE04\uD83D\uDE07\uD83D\uDC7A""#;
        let token = Lexer::new(&s).parse_string().unwrap();
        assert_eq!(token, Token::String(r#"ğŸ˜„ğŸ˜‡ğŸ‘º"#.to_string()));
    }


    #[test]
    fn test_get_string() {
        let expect = String::from("test");
        let mut lexer = Lexer::new("test");
        let actual = lexer.get_string(4);
        assert_eq!(actual, expect);
    }

    #[test]
    fn test_is_number() {
        assert_eq!(is_number('1', true), true);
        assert_eq!(is_number('+', true), true);
        assert_eq!(is_number('e', true), false);
        assert_eq!(is_number('e', false), true);
        assert_eq!(is_number('a', false), false);
    }

    #[test]
    fn test_lexical_analyze() {
        let obj = r#"
        {
            "number": 123,
            "boolean": true,
            "string": "togatoga",
            "object": {
               "number": 2E10
            }
         }
         "#;
        // object
        let tokens = Lexer::new(obj).lexical_analyze().unwrap();
        let result_tokens = [
            // start {
            Token::LeftBrace,
            // begin: "number": 123,
            Token::String("number".to_string()),
            Token::Colon,
            Token::Number(123f64),
            Token::Comma,
            // end

            // begin: "boolean": true,
            Token::String("boolean".to_string()),
            Token::Colon,
            Token::Bool(true),
            Token::Comma,
            // end

            // begin: "string": "togatoga",
            Token::String("string".to_string()),
            Token::Colon,
            Token::String("togatoga".to_string()),
            Token::Comma,
            // end

            // begin: "object": {
            Token::String("object".to_string()),
            Token::Colon,
            Token::LeftBrace,
            // begin: "number": 2E10,
            Token::String("number".to_string()),
            Token::Colon,
            Token::Number(20000000000f64),
            // end
            Token::RightBrace,
            // end
            Token::RightBrace,
            // end
        ];
        tokens
            .iter()
            .zip(result_tokens.iter())
            .enumerate()
            .for_each(|(i, (x, y))| {
                assert_eq!(x, y, "index: {}", i);
            });

        // array
        let a = "[true, {\"ã‚­ãƒ¼\": null}]";
        let tokens = Lexer::new(a).lexical_analyze().unwrap();
        let result_tokens = vec![
            Token::LeftBracket,
            Token::Bool(true),
            Token::Comma,
            Token::LeftBrace,
            Token::String("ã‚­ãƒ¼".to_string()),
            Token::Colon,
            Token::Null,
            Token::RightBrace,
            Token::RightBracket,
        ];
        tokens
            .iter()
            .zip(result_tokens.iter())
            .for_each(|(x, y)| assert_eq!(x, y));
    }
}